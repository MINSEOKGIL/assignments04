

from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.functions import col, count, row_number, lit, current_timestamp
from pyspark.sql.window import Window
from datetime import datetime
import sys


def create_spark_session(app_name="TopN Calculator"):

    spark = SparkSession.builder \
        .appName(app_name) \
        .master("spark://spark-master:7077") \
        .config("spark.jars", "/opt/spark/extra-jars/postgresql-42.7.1.jar") \
        .config("spark.sql.adaptive.enabled", "true") \                
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.driver.memory", "1g") \
        .config("spark.executor.memory", "2g") \
        .config("spark.executor.cores", "2") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    return spark


def read_clickstream_data(spark, start_date, end_date):
    
    jdbc_url = "jdbc:postgresql://postgres:5432/clickdb"
    connection_properties = {
        "user": "admin",
        "password": "admin",
        "driver": "org.postgresql.Driver"
    }

    # ë‚ ì§œì¸ì§€, ì‹œê°„ê¹Œì§€ í¬í•¨ëœ ë¬¸ìì—´ì¸ì§€ íŒë‹¨
    if "T" in start_date or " " in start_date:
        start_ts = start_date
        end_ts = end_date
    else:
        start_ts = f"{start_date} 00:00:00"
        end_ts = f"{end_date} 23:59:59"

    query = f"""
        (SELECT 
            event_time,
            event_type,
            product_id,
            category_id,
            category_code,
            brand,
            price,
            user_id,
            user_session
        FROM user_clickstream
        WHERE event_time >= '{start_ts}'::timestamp
          AND event_time < '{end_ts}'::timestamp
          AND event_type = 'view'
          AND product_id IS NOT NULL
        ) AS clickstream_data
    """

    print(f"ğŸ“Š ë°ì´í„° ì½ê¸°: {start_ts} ~ {end_ts}")

    df = spark.read.jdbc(url=jdbc_url, table=query, properties=connection_properties)
    print(f"âœ… ì½ì€ ë ˆì½”ë“œ ìˆ˜: {df.count():,}ê±´")
    return df


def calculate_topn(spark, df, top_n=10):
    print(f"ğŸ”¢ TopN ê³„ì‚° ì¤‘... (ìƒìœ„ {top_n}ê°œ)")

    # JDBCRelation â†’ InMemoryRelation (ì—ëŸ¬ ë°©ì§€)
    df = df.cache()

    product_counts = df.groupBy("product_id").agg(
        F.count("*").alias("view_count"),
        F.first("brand", ignorenulls=True).alias("brand_sample"),
        F.first("category_code", ignorenulls=True).alias("category_sample")
    )

    window_spec = Window.orderBy(col("view_count").desc())

    topn_df = product_counts \
        .withColumn("rank", row_number().over(window_spec)) \
        .filter(col("rank") <= top_n) \
        .orderBy("rank")

    return topn_df


def save_daily_topn(topn_df, date_str):
    jdbc_url = "jdbc:postgresql://postgres:5432/clickdb"
    connection_properties = {
        "user": os.getenv("DB_USER"),
        "password": os.getenv("DB_PASSWORD"),
        "driver": "org.postgresql.Driver"

        
    }

    result_df = topn_df \
        .withColumn("date", lit(date_str)) \
        .withColumn("created_at", current_timestamp()) \
        .select("date", "rank", "product_id", "view_count",
                "brand_sample", "category_sample", "created_at")

    print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘... (daily_topn_results)")

    result_df.write.jdbc(
        url=jdbc_url,
        table="daily_topn_results",
        mode="append",
        properties=connection_properties
    )

    print(f"âœ… {date_str} TopN ì €ì¥ ì™„ë£Œ!")


def save_period_topn(topn_df, start_date, end_date):
    jdbc_url = "jdbc:postgresql://postgres:5432/clickdb"
    connection_properties = {
        "user": os.getenv("DB_USER"),
        "password": os.getenv("DB_PASSWORD"),
        "driver": "org.postgresql.Driver"
    }

    result_df = topn_df \
        .withColumn("period_start", lit(start_date)) \
        .withColumn("period_end", lit(end_date)) \
        .withColumn("created_at", current_timestamp()) \
        .select("period_start", "period_end", "rank", "product_id",
                "view_count", "brand_sample", "category_sample", "created_at")

    print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘... (period_topn_results)")

    result_df.write.jdbc(
        url=jdbc_url,
        table="period_topn_results",
        mode="append",
        properties=connection_properties
    )

    print(f"âœ… {start_date} ~ {end_date} TopN ì €ì¥ ì™„ë£Œ!")


def run_daily_topn(date_str, top_n=10):
    print(f"\n{'='*60}")
    print(f"ğŸ“… Daily TopN ì‹¤í–‰: {date_str}")
    print(f"{'='*60}\n")

    spark = None
    try:
        spark = create_spark_session(f"Daily TopN - {date_str}")
        df = read_clickstream_data(spark, date_str, date_str)

        if df.count() == 0:
            print(f"âš ï¸ {date_str}ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        topn_df = calculate_topn(spark, df, top_n)

        print(f"\nğŸ† {date_str} TopN ê²°ê³¼:")
        topn_df.select("rank", "product_id", "view_count",
                       "brand_sample", "category_sample").show(top_n, truncate=False)

        save_daily_topn(topn_df, date_str)
        print(f"\nâœ… Daily TopN ì™„ë£Œ!")

    except Exception as e:
        print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        raise
    finally:
        if spark:
            spark.stop()


def run_period_topn(start_date, end_date, top_n=10):
    print(f"\n{'='*60}")
    print(f"ğŸ“… Period TopN ì‹¤í–‰: {start_date} ~ {end_date}")
    print(f"{'='*60}\n")

    spark = None
    try:
        spark = create_spark_session(f"Period TopN - {start_date} to {end_date}")
        df = read_clickstream_data(spark, start_date, end_date)

        if df.count() == 0:
            print(f"âš ï¸ {start_date} ~ {end_date} ê¸°ê°„ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        topn_df = calculate_topn(spark, df, top_n)

        print(f"\nğŸ† {start_date} ~ {end_date} TopN ê²°ê³¼:")
        topn_df.select("rank", "product_id", "view_count",
                       "brand_sample", "category_sample").show(top_n, truncate=False)

        save_period_topn(topn_df, start_date, end_date)
        print(f"\nâœ… Period TopN ì™„ë£Œ!")

    except Exception as e:
        print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        raise
    finally:
        if spark:
            spark.stop()


if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•:")
        print("  ì¼ì¼ TopN: python spark_topn_job.py daily YYYY-MM-DD [top_n]")
        print("  ê¸°ê°„ TopN: python spark_topn_job.py period YYYY-MM-DD YYYY-MM-DD [top_n]")
        sys.exit(1)

    mode = sys.argv[1]

    if mode == "daily":
        date_str = sys.argv[2]
        top_n = int(sys.argv[3]) if len(sys.argv) > 3 else 10
        run_daily_topn(date_str, top_n)

    elif mode == "period":
        start_date = sys.argv[2]
        end_date = sys.argv[3]
        top_n = int(sys.argv[4]) if len(sys.argv) > 4 else 10
        run_period_topn(start_date, end_date)

    else:
        print(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë“œ: {mode}")
        sys.exit(1)
