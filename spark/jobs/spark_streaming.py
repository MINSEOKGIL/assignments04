from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col, window, count, desc, to_timestamp
from pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType
import logging, sys, os


# âœ… ë¡œê±° ì„¤ì • í•¨ìˆ˜
def setup_logger():
    os.makedirs("logs", exist_ok=True)
    logger = logging.getLogger("SparkViewConsumer")
    logger.setLevel(logging.INFO)
    formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')

    file_handler = logging.FileHandler("logs/spark_view_consumer.log", mode='a', encoding='utf-8')
    file_handler.setFormatter(formatter)

    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)

    if not logger.handlers:
        logger.addHandler(file_handler)
        logger.addHandler(console_handler)

    return logger


logger = setup_logger()


#  Spark ì„¸ì…˜ ìƒì„±
def create_spark_session():
    try:
        spark = (
            SparkSession.builder
            .appName("KafkaViewConsumer")
            .master("spark://spark-master:7077")
            .config("spark.sql.shuffle.partitions", "4")
            .getOrCreate()
        )
        logger.info("âœ… Spark session initialized successfully.")
        return spark
    except Exception as e:
        logger.exception(f"âŒ Spark session initialization failed: {e}")
        raise


#  Kafka ë©”ì‹œì§€ êµ¬ì¡° ì •ì˜ (schema + payload)
payload_schema = StructType([
    StructField("event_time", StringType(), True),
    StructField("event_type", StringType(), True),
    StructField("product_id", LongType(), True),
    StructField("category_id", LongType(), True),
    StructField("category_code", StringType(), True),
    StructField("brand", StringType(), True),
    StructField("price", FloatType(), True),
    StructField("user_id", LongType(), True),
    StructField("user_session", StringType(), True)
])

schema = StructType([
    StructField("schema", StringType(), True),
    StructField("payload", payload_schema, True)
])


#  ë©”ì¸ ë¡œì§
def main():
    try:
        spark = create_spark_session()

        # Kafkaì—ì„œ ë©”ì‹œì§€ ì½ê¸°
        df_raw = (
            spark.readStream
            .format("kafka")
            .option("kafka.bootstrap.servers", "kafka1:9092,kafka2:9093")
            .option("subscribe", "user_clickstream")
            .option("startingOffsets", "latest")
            .option("maxOffsetsPerTrigger", 10000) 
            .load()
        )

       
        df_json = df_raw.select(from_json(col("value").cast("string"), schema).alias("data"))
        df = df_json.select("data.payload.*")

        # ì´ë²¤íŠ¸ ì‹œê°„ íƒ€ì… ë³€í™˜ ë° view ì´ë²¤íŠ¸ í•„í„°ë§
        df = df.withColumn("event_time", to_timestamp(col("event_time"), "yyyy-MM-dd HH:mm:ss"))
        df = df.filter(col("event_type") == "view")

        #  Watermark ì„¤ì •: 1ë¶„ ì§€ì—°ëœ ë°ì´í„°ê¹Œì§€ í—ˆìš©
        df_watermarked = df.withWatermark("event_time", "1 minutes")
        
        #  Watermark ê¸°ë°˜ ìƒíƒœ ê´€ë¦¬ ì¤‘ë³µ ì œê±°:
        # (user_session, product_id, event_time) ì¡°í•©ì´ Watermark ê¸°ê°„ ë™ì•ˆ ê³ ìœ í•¨ì„ ë³´ì¥ (Exactly-Once)
        df_deduped = df_watermarked.dropDuplicates(
            ["user_session", "product_id", "event_time"]
        )

        # ìœˆë„ìš° ì§‘ê³„
        windowed_counts = (
            df_deduped
              .groupBy(
                  window(col("event_time"), "3 minutes"),
                  col("product_id")
              )
              .agg(count("*").alias("view_count"))
        )


        #  Top-N ì •ë ¬
        topn = (
            windowed_counts
            .orderBy(col("window.start").asc(), desc("view_count"))
            .limit(10)
        )

        #  ì½˜ì†” ì¶œë ¥
        query = (
            topn.writeStream
            .outputMode("complete") 
            .format("console")
            .option("truncate", "false")
            .option("numRows", 10)
            .trigger(processingTime="30 seconds")
            .option("checkpointLocation", "checkpoint/dir")
            .start()
        )

        logger.info("ğŸš€ Spark streaming started successfully with watermark.")
        query.awaitTermination()

    except Exception as e:
        logger.exception(f"âŒ Streaming process failed: {e}")

    finally:
        try:
            spark.stop()
            logger.info("ğŸ›‘ Spark session stopped safely.")
        except Exception as e:
            logger.warning(f"âš ï¸ Spark session stop failed: {e}")


if __name__ == "__main__":
    main()
