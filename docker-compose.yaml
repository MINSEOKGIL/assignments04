
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    ports:
      - "2181:2181"
   
    networks:
      - kafka-net

   
  

  kafka1:
    image: confluentinc/cp-kafka:7.6.0
    container_name: kafka1
    depends_on:
       - zookeeper
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: INTERNAL://0.0.0.0:9092,EXTERNAL://0.0.0.0:29092
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka1:9092,EXTERNAL://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2
      KAFKA_MIN_INSYNC_REPLICAS: 1
    command: >
      bash -c "(/etc/confluent/docker/run &) && sleep 10 && kafka-topics --create --if-not-exists --topic user_clickstream --bootstrap-server kafka1:9092 --replication-factor 2 --partitions 3 && tail -f /dev/null"

    restart: always
    networks:
      - kafka-net



  kafka2:
    image: confluentinc/cp-kafka:7.6.0
    container_name: kafka2
    depends_on:
       - zookeeper
    ports:
      - "9093:9093"     # 내부용 (도커 컨테이너 간)
      - "29093:29093"
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: INTERNAL://0.0.0.0:9093,EXTERNAL://0.0.0.0:29093
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka2:9093,EXTERNAL://localhost:29093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2
      KAFKA_MIN_INSYNC_REPLICAS: 1
    restart: always

    networks:
      - kafka-net


  kafka3:
    image: confluentinc/cp-kafka:7.6.0
    container_name: kafka3
    depends_on:
       - zookeeper
    ports:
      - "9094:9094"     # 내부용 (도커 컨테이너 간)
      - "29094:29094"
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: INTERNAL://0.0.0.0:9094,EXTERNAL://0.0.0.0:29094
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka3:9094,EXTERNAL://localhost:29094
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2
      KAFKA_MIN_INSYNC_REPLICAS: 1
    restart: always

    networks:
      - kafka-net    

  postgres:
    image: postgres:15-alpine
    container_name: postgres
    environment:
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: admin
      POSTGRES_DB: clickdb
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql  
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U admin -d clickdb"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: always
    networks:
      - kafka-net

  kafka-connect:
    build:
      context: .                     # 현재 디렉터리
      dockerfile: Dockerfile.kafka-connect
    container_name: kafka-connect
    depends_on:
      - kafka1
      - kafka2
      - postgres
    ports:
      - "8083:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: kafka1:9092,kafka2:9093
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: "connect-cluster"
      CONNECT_CONFIG_STORAGE_TOPIC: "connect-configs"
      CONNECT_OFFSET_STORAGE_TOPIC: "connect-offsets"
      CONNECT_STATUS_STORAGE_TOPIC: "connect-status"
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: "2"
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: "2"
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: "2"
      CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.storage.StringConverter"
      CONNECT_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "true"
      CONNECT_REST_ADVERTISED_HOST_NAME: "kafka-connect"
      
      # PostgreSQL JDBC Driver
   
      # PostgreSQL 연결정보 (Kafka Connect에서 접근)
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      POSTGRES_DB: clickdb
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: admin

    networks:
      - kafka-net
    restart: always
   


  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    depends_on:
      - kafka1
      - kafka2
      - kafka3
    ports:
      - "8089:8080"
    environment:
      DYNAMIC_CONFIG_ENABLED: 'true'
      KAFKA_CLUSTERS_0_NAME: local-cluster
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka1:9092,kafka2:9093,kafka3:9094
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
    restart: always
    networks:
      - kafka-net



  # ------------------------------------------
  # 1. Airflow 환경 초기화 (DB 및 Admin User 생성)
  # ------------------------------------------
  airflow-init:
    image: apache/airflow:2.8.1-python3.10
    container_name: airflow-init
    # Spark 관련 패키지를 설치합니다 (Airflow에서 SparkSubmitOperator 등을 사용하기 위함)
    entrypoint: /bin/bash
    command: -c "
      
      airflow db init &&
      airflow users create \
        --username admin \
        --firstname Admin \
        --lastname User \
        --role Admin \
        --email admin@example.com \
        --password admin
      "
    environment:
      - AIRFLOW_HOME=/opt/airflow
    volumes:
      - airflow_data:/opt/airflow
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./spark/jobs:/opt/spark/jobs 
    networks:
      - kafka-net

  # ------------------------------------------
  # 2. Airflow Webserver (UI 접속)
  # ------------------------------------------
  airflow-webserver:
    image: apache/airflow:2.8.1-python3.10
    container_name: airflow-webserver
    depends_on:
      - airflow-init
    ports:
      - "8080:8080" # Airflow UI는 8080 포트를 사용
    environment:
      - AIRFLOW_HOME=/opt/airflow
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor # 개발용으로 SQLite와 호환되는 SequentialExecutor 사용
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
    volumes:
      - airflow_data:/opt/airflow
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./spark/jobs:/opt/spark/jobs 
    command: webserver
    restart: always
    networks:
      - kafka-net

  # ------------------------------------------
  # 3. Airflow Scheduler (DAG 스케줄 관리)
  # ------------------------------------------
  airflow-scheduler:
    image: apache/airflow:2.8.1-python3.10
    container_name: airflow-scheduler
    depends_on:
      - airflow-webserver
      - spark-master
    environment:
      - AIRFLOW_HOME=/opt/airflow
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor # 개발용으로 SQLite와 호환되는 SequentialExecutor 사용
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
    volumes:
      - airflow_data:/opt/airflow
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./spark/jobs:/opt/spark/jobs 
    command: scheduler
    restart: always
    networks:
      - kafka-net
  



  # ------------------------------------------
  # 4. Spark Master Node
  # ------------------------------------------
  spark-master:
    image: apache/spark:3.5.1
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_NO_DAEMONIZE=true        # 로그가 foreground로 출력되어 Docker 로그로 확인 가능
      - SPARK_LOG_DIR=/opt/spark/logs  # 로그 저장 경로 지정
      - SPARK_MASTER_WEBUI_PORT=8081
    command: >
      bash -c "/opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
               --host spark-master
               --port 7077
               --webui-port 8081"
    ports:
      - "7077:7077"    # Worker 통신용
      - "8081:8081"    # Master Web UI
    networks:
      - kafka-net
    volumes:
      - ./spark/jobs:/opt/spark/jobs   # Spark 배치 스크립트 저장용
      - ./spark/logs:/opt/spark/logs
      - ./spark/jobs:/opt/spark/app      # ✅ 로컬 코드 자동 반영
      - ./spark/extra-jars:/opt/spark/extra-jars
    restart: always

  
  
  spark-worker:
    image: apache/spark:3.5.1
    container_name: spark-worker
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_WORKER_CORES=2           # CPU 코어 수 제한
      - SPARK_WORKER_MEMORY=2G         # 메모리 제한
      - SPARK_NO_DAEMONIZE=true
      - SPARK_LOG_DIR=/opt/spark/logs
    command: >
      bash -c "/opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077 --webui-port 8082"
    ports:
      - "8082:8082"    # Worker Web UI
    networks:
      - kafka-net
    volumes:
      - ./spark/jobs:/opt/spark/jobs
      - ./spark/logs:/opt/spark/logs
      - ./spark/jobs:/opt/spark/app
      - ./spark/extra-jars:/opt/spark/extra-jars     

    restart: always


  spark-worker-2:
    image: apache/spark:3.5.1
    container_name: spark-worker-2
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2G
      - SPARK_NO_DAEMONIZE=true
      - SPARK_LOG_DIR=/opt/spark/logs
    command: >
      bash -c "/opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077 --webui-port 8083"
    ports:
      - "8085:8085"
    networks:
      - kafka-net
    volumes:
      - ./spark/jobs:/opt/spark/jobs
      - ./spark/logs:/opt/spark/logs
      - ./spark/jobs:/opt/spark/app
      - ./spark/extra-jars:/opt/spark/extra-jars
    restart: always      
  
  spark-worker-3:
    image: apache/spark:3.5.1
    container_name: spark-worker-3
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2G
      - SPARK_NO_DAEMONIZE=true
      - SPARK_LOG_DIR=/opt/spark/logs
    command: >
      bash -c "/opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077 --webui-port 8083"
    ports:
      - "8086:8086"
    networks:
      - kafka-net
    volumes:
      - ./spark/jobs:/opt/spark/jobs
      - ./spark/logs:/opt/spark/logs
      - ./spark/jobs:/opt/spark/app
      - ./spark/extra-jars:/opt/spark/extra-jars
    restart: always




networks:
  kafka-net:
    driver: bridge
volumes:
  postgres_data:
  airflow_data:



